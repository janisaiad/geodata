\section{Implementation}
\label{sec-implementation}

We detail here how to compute all divergences defined Section~\ref{sec-ot-prop} when $(\al,\be)$ are discrete measures.
It takes two steps.
Firstly $(\f_{\al\be},\g_{\al\be}, \f_\al, \g_\be)$ are computed using the Sinkhorn algorithm~\ref{def-sinkhorn}.
Secondly, potentials are summed against the input measures as described for instance in Proposition~\ref{prop-funct-kl} when $\D_\phi=\rho\KL$.

\subsection{Sinkhorn algorithm}

\paragraph{Discrete Setting}
We write discrete measures as $\al = \sum_{i=1}^\N \AL_i \de_{x_i}$ and $\be=\sum_{j=1}^\M \BE_j \de_{y_j}$, where $(\AL_i)_i,(\BE_j)_j \in\R_+^\N$ are vectors of non-negative masses and $(x_i)_{i},(y_j)_{j} \in \Xx^\N$ are two sets of points.
Potentials $(\f_i) = (\f(x_i))$ and $(\g_j) = (\g(y_j))$ become two vectors of $\R^\N$ and $\R^\M$.
The cost $\C_{ij}=\C(x_i,y_j)$ and the transport plan $\pi_{ij}=\pi(x_i,y_j)$ become matrices of $\R^{\N\times\M}$.
The latter can be computed with Equation~\eqref{eq-implicit-plan} which becomes $\pi_{ij} =  \exp\tfrac{1}{\epsilon}[\f_i + \g_j - \C_{ij}]\al_i \be_j$.
Once potentials $(\f,\g)$ are computed by the Sinkhorn algorithm, functionals of Definition~\ref{def-sink-div-unb} involve discrete sums such as $\dotp{\al}{\phi^*(-\f)} = \sum_{i=1}^N \AL_i \phi^*(-\f_i)$.
%As discussed in \cite[Section 3.3.1]{feydy2020thesis}, 
%we write our Sinkhorn iterations in the log-domain and work directly with the dual potentials $\f$ and $\g$, sampled on the $x_i$'s and $y_j$'s.
%This is in sharp contrast with a majority of implementations of the Sinkhorn algorithm -- such as~\cite{chizat2016scaling} -- that rely
%on exponentiated variables $a = \exp(\f/\epsilon)$ and $b = \exp(\g /\epsilon)$
%and are numerically unstable for small values of $\epsilon$.



\paragraph{Computational routines}
The Sinkhorn algorithm allows parallel computations, and is thus ideally suited to modern computing hardware (e.g. GPU).
In practice, we rely on the standard NumPy~\cite{numpy} and PyTorch~\cite{pytorch} libraries for array manipulations and display our results using Matplotlib~\cite{matplotlib}.
When using GPUs, we rely on the KeOps library~\cite{charlier2020kernel,feydy2020fast} to perform fast computations, with a negligible memory footprint -- which is often a bottleneck with GPUs.

Contrary to~\cite{chizat2016scaling} where $\proxdiv{\phi}$ is used (see Section~\ref{sec-operators}), the use of $\aprox{\phi^*}$ allows to perform iterations on \emph{log-domain}.
We update $(\f,\g)$ instead of $(e^{\f / \epsilon}, e^{\g/\epsilon})$, which is key for numerical stability.
Indeed, operators $(\Ss_\al,\Ss_\be)$ (see Equation~\eqref{eq-defn-softmin-func}) are Log-Sum-Exp reductions, an operation which can be stabilized as
%In the log-domain, the main operation behind the Sinkhorn updates is the ``Softmax'' or Log-Sum-Exp reduction of vectors $(u_i) \in \R^\N$ that reads
\begin{align*}
	\LSE_{i=1}^\N (u_i) \eqdef \log\textstyle\sum_{i=1}^\N \exp(u_i) =  \max_{k} u_k + \log\textstyle\sum_{i=1}^\N \exp(u_i - \max_{k} u_k).
\end{align*}
Such expression avoids numerical overflows of exponentials since $u_i - \max_{k} u_k\leq 0$.
It also avoids underflows in the sense that updates of $(e^{\f / \epsilon}, e^{\g/\epsilon})$ involve the matrix $(e^{-\C_{ij}/\epsilon})$ whose coordinates are numerically underflowing to $0$ for small $\epsilon$.
%In the expression above, factoring the largest term out of the summation guarantees numerical stability: 
%it ensures that all the exponentiated terms are smaller than $0$, with (at least) one equality.

%In practice, 
%As discussed in \cite[Section 2.2.4]{feydy2020thesis}, this extension for PyTorch, NumPy and other frameworks relies on a running  maximum to avoid looping twice on the values of $u_i$ and outperforms baseline GPU implementations by several orders of magnitude.

\paragraph{Algorithm}
We detail the implementation in Algorithm~\ref{alg:sinkhorn}.
We emphasize that the only change from \emph{balanced} Sinkhorn is the extra composition with the operator $\aprox{\phi^*}$ with the Log-Sum-Exp reduction.
As detailed in Section~\ref{sec-exmp-f-div}, $\aprox{\phi^*}$ is often cheap too compute, thus not impacting the computation cost of Sinkhorn.

%With these computational tools at hand, the unbalanced Sinkhorn algorithm can then be implemented as a simple generalization of the standard Sinkhorn loop. 
%it differs only by the application of the $\aprox{\phi^*}$ operator after the Log-Sum-Exp computations. 
%As detailed in Table~\ref{tab-entropies-aprox} and illustrated in Figure~\ref{fig-aprox}, this coordinate-wise operation is usually available in closed form and straightforward to implement.



{\centering
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
	\caption{~~~~\, {Sinkhorn  Algorithm: Sink($(\AL_i)_i$, $(x_i)_i$, $(\BE_j)_j$, $(y_j)_j$)} \label{alg:sinkhorn}}
	\textbf{Parameters~:}~~ symmetric cost function $\C(x,y)$, regularization $\epsilon > 0$ \\
	\textbf{Input~~~~\,:}~~ source $\alpha = \sum_{i=1}^\N \AL_i\delta_{x_i}$,~target~ $\beta = \sum_{j=1}^\M \BE_j\delta_{y_j}$\\
	\textbf{Output~~\,:}~~ vectors $(\f_i)_i$ and $(\g_j)_j$, equal to the optimal potentials\\
	\vspace*{-1.0em}
	\begin{algorithmic}[1]
		\STATE $\f_i\gets \text{zeros}(\M)$~~;~~$\g_j\gets \text{zeros}(\N)$ \COMMENT{Vectors of size $\M$ and $\N$}
		\vspace{.1cm}\WHILE{updates $>$ tol}\vspace{.1cm}
		\STATE $\g_j \gets - \,\epsilon \LSE_{i=1}^\N \big[ \log(\AL_i) + (\f_i - \C(x_i,y_j))\,/\,\epsilon\,\big]$
		\STATE $\g_j \gets -\aprox{\phi^*}(-\g_j)$
		\label{alg:sinkhorn:line_a}
		\STATE $\f_i \gets - \,\epsilon \LSE_{j=1}^\M \big[ \log(\BE_j) + (\g_j - \C(x_i,y_j))\,/\,\epsilon\,\big]$
		\STATE $\f_i \gets -\aprox{\phi^*}(-\f_i)$
		\label{alg:sinkhorn:line_b}
		\ENDWHILE
		\RETURN{~~$(\f_i)_i,~~(\g_j)_j$}
	\end{algorithmic}
\end{algorithm}
\end{minipage}
}

\begin{remark}
	It is possible to implement Remark~\ref{rem-extrapolate-pot} to extrapolate potentials, which matters to compute the Hausdorff divergence.
	For instance, take $(f_i)_i$ s.t. $\f=\Aa\Ss_\al(\f)$ with $\al = \sum_{i=1}^\N \AL_i \de_{x_i}$.
	To evaluate at some $y$, we compute
	\begin{align*}
		\f(y) = -\aprox{\phi^*}\big(\epsilon\log \sum_{i=1}^\N \exp \big[ \log(\AL_i) + (\f_i - \C(x_i,y))\,/\,\epsilon\,\big]\big).
	\end{align*}
\end{remark}
%\paragraph{Initialization of the dual potentials.}
%\todo{remove ?}
%We have shown that the Sinkhorn algorithm converges towards a global optimum for any initialization of the dual vectors $(\f_i)$ and $(\g_j)$. In practice however, taking $\f_0=\g_0=0$ as suggested in the algorithm above may not be the most clever option. Following the simulated annealing heuristic (also called $\epsilon$-scaling in the context of the auction algorithm and entropic OT~\cite{Schmitzer2016}), we remark that initializing the algorithm with the asymptotic solution ``for $\epsilon\rightarrow+\infty$'' often leads to faster convergence. Closed form expressions in the high temperature setting can also provide reference points for debugging purposes. 
%Informally, the optimality condition $\dotp{\be}{e^{\f\oplus\g-\C / \epsilon}}=\nabla\phi^*(-\f)$ becomes $m(\be)=\nabla\phi^*(-\f)$, or equivalently $\f=-\nabla\phi(m(\be))$. 
%
%For the sake of convenience, we provide below a list of asymptotic identities ``at $\epsilon = +\infty$'' for the main settings of Section~\ref{sec-exmp-f-div}.
%These formulas can be used as sensible initializations for both dual vectors $\f$
%and $\g$ -- switching the roles of $\al$ and $\be$ in the second case:
%\begin{itemize}
%\item \textbf{Balanced:} $\f_0 =  \C\star\be - \tfrac{1}{2}\dotp{\al}{\C\star\be}$.
%\item \textbf{Kullback--Leibler:} $\f_0 = -\rho\log(m(\be))$.
%\item \textbf{Range:} $\f_0 = 0$.
%\item \textbf{Total Variation:}  In that case there is no explicit formula, but a satisfying approximation is $\f_0 =  -\rho\,\text{sign}(\log(m(\be)))$ when $m(\be)\neq 1$, and
%$\f_0 =  -\aprox{\phi^*}(-\C\star\be + \tfrac{1}{2}\dotp{\al}{\C\star\be})$ otherwise. 
%\item \textbf{Power entropy:} $\f_0 = \rho(1-r)(m(\be)^{\frac{1}{r - 1} } - 1) $, where $r$ is the dual exponent associated to $\phi^*$.
%\end{itemize}

%\subsection{Computing the divergences}
%\todo{Remove this section or refactor}

%Once optimal potentials $(\f,\g)$ are obtained, we compute all functionals of Definition~\ref{def-sink-div-unb} using the expression
%%
%\begin{align}\label{eq-discret-num}
%  \dotp{\al}{\phi^*(-\f)} = \sum_{i=1}^N \AL_i \phi^*(-\f_i).
%\end{align}
%
%There is one subtlety concerning the Hausdorff divergence. 
%The Sinkhorn algorithm computes vectors $(\f_i)_i$ and $(\g_j)_j$ that are made out of samples $f(x_j)$ and $g(y_j)$ of the continuous dual potentials $\f$ and $\g$ on the supports of $\al$ and $\be$, respectively. 
%To evaluate these functions \emph{outside} of the support, e.g. compute $f$ on the support of $\be$,
%we apply Remark~\ref{rem-extrapolate-pot} and write
%\begin{align*}
%  \f(y_j) = -\aprox{\phi^*}\big(\epsilon\log \sum_{i=1}^\N \exp \big[ \log(\AL_i) + (\f_i - \C(x_i,y_j))\,/\,\epsilon\,\big]\big).
%\end{align*}
%%
%With this extra step in mind, we can then compute the Hausdorff divergence with~\eqref{eq-discret-num}.

