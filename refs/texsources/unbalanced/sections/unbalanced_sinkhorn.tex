\subsection{Bounds and  Asymptotics of $\Sb$}
\label{subsec-sink-div}

We present properties of $\Sb$ which are key to prove the main Theorems~\ref{thm-sink-unb} and~\ref{thm-sink-weak-cv}.
We provide two bounds.
The first one involves $\Hb$ and extends~\cite{feydy2018interpolating}.
The second is new and involves a kernel norm, thus hilighting the connection between entropic OT and Reproducing Kernel Hilbert Spaces (RKHS).

%We now present some properties of the Sinkhorn divergence that shed some light on its geometric behaviour and are required to prove our positivity results.
%Notably, we provide two lower bounds of $\Sb$: the first one is a generalization of the Hausdorff divergence bound of~\cite{feydy2018interpolating}; the second bound involves a kernel norm and highlights a connection between entropized OT and Reproducing Kernel Hilbert Spaces (RKHS).




\begin{proposition}[The Sinkhorn divergence is bounded from below by a ``soft'' Hausdorff divergence]\label{prop-Seps-ineq-bregman}
Under Assumptions~\ref{as:1} and \ref{as:2}, for any $(\al,\be)\in\Mmpp(\Xx)$, one has
\begin{align*}
  \Sb(\al,\be) \geq \tfrac{1}{2} \Hb(\al,\be)\geq 0.
\end{align*}
\end{proposition}
\begin{proof}
The functional $\OTb$ is convex in $\al$ and in $\be$. 
Theorem~\ref{thm-diff-unb} holds, Thus $\OTb$ is differentiable. 
The first order convexity inequality gives
\begin{align*}
  &\OTb(\al,\be) \geq \OTb(\be,\be) + \dotp{\al-\be}{\nabla_1 \OTb(\be,\be)}\\
  &\OTb(\al,\be) \geq \OTb(\al,\al) + \dotp{\be-\al}{\nabla_2 \OTb(\al,\al)}\,.
\end{align*}
Applying Theorem~\ref{thm-diff-unb} and Lemma~\ref{lem-sym-pot}, gradients $\nabla_1 \OTb(\be,\be)$ and $\nabla_2 \OTb(\al,\al)$ verify $\nabla\Fb(\al) = -\nabla_1 \OTb(\al,\al) + \epsilon m(\al)=-\nabla_2 \OTb(\al,\al) + \epsilon m(\al)$.
Summing the above inequalities thus yields
\begin{align*}
   2 \OTb(\al,\be) &\geq \OTb(\al,\al) + \OTb(\be,\be) \\
  &+  \dotp{\al-\be}{-\nabla\Fb(\be) +\epsilon m(\be)} + \dotp{\be-\al}{-\nabla\Fb(\al) +\epsilon m(\al)},\\
   2 \OTb(\al,\be) &\geq \OTb(\al,\al) + \OTb(\be,\be) \\
   &+ \dotp{\al - \be}{\nabla\Fb(\al) - \nabla\Fb(\be)} - \epsilon (m(\al) - m(\be))^2,\\
  \Sb(\al,\be) &\geq \tfrac{1}{2} \Hb(\al,\be).
\end{align*}
Finally, we apply Proposition~\ref{thm-Feps_uniqueness}. 
The Hausdorff divergence is a Bregman divergence associated to $\Fb$ which is convex. 
Thus $\Hb$ is positive. 
\end{proof}

\begin{proposition}[The Sinkhorn divergence is bounded from below by a kernel norm]\label{prop-Seps-ineq-norm}
For any entropy $\phi$, write $(\f_\al,\g_\be)$ optimal symmetric potentials for $\Fb(\al)$ and $\Fb(\be)$. 
Then, one has
\begin{align}%\label{positivity-unb}
\Sb(\al,\be) \geq \tfrac{\epsilon}{2} \Vert  \al e^{\frac{\f_{\al}}{\epsilon}} - \be e^{\frac{\g_{\be}}{\epsilon}} \Vert^2_{k_\epsilon}.
\end{align}
\end{proposition}
\begin{proof}
The pair $(\f_\al,\g_\be)$ is suboptimal in $\OTb(\al,\be)\geq\Ff(\f_\al,\g_\be)$. The detailed calculation is deferred in Appendix~\ref{appendix-proofs}.
\end{proof}

\begin{remark}\label{rem-bound-sinkdiv}
	When $(\al,\be)=(\de_x,\de_y)$, the first bound is sharp, while the second is not and approaches $0$ as $\C(x,y)\rightarrow\infty$.
%	We note that the first bound is sharp when measures are Diracs, while the second bound is not and saturates when the measures' supports are far away from each other. In this sense, Proposition~\ref{prop-Seps-ineq-bregman} can be interpreted as a geometric, ``global'' bound whereas the result of Proposition~\ref{prop-Seps-ineq-norm} is purely local and related to the smoothing introduced by entropic regularization.
\end{remark}

Finally, we show how the entropic regularization impacts the behaviour of $\Sb$ when $\epsilon\rightarrow\infty$.

\begin{proposition}[Behaviour of the Sinkhorn divergence when $\epsilon$ tends to infinity]\label{prop-asymptotic-unb}
For any entropy $\phi$, any measures $(\al,\be)$ such that $(m(\al),m(\be))\in \text{\upshape dom}(\phi)$. 
One has when $\epsilon\rightarrow\infty$,
\begin{align*}
  \OTb(\al,\be) \rightarrow\, &\dotp{\al}{\C\star\be} + m(\al)\phi(m(\be)) + m(\be)\phi(m(\al)).\\
   \Sb(\al,\be) =  &\norm{\al - \be}^2_{-\C} + (m(\al) - m(\be))(\phi(m(\be)) - \phi(m(\al)))\\
  &+ \tfrac{\epsilon}{2}(m(\al) - m(\be))^2  + o(1).
\end{align*}
\end{proposition}
\begin{proof}
The plan $\pi=\al\otimes\be$ is suboptimal in the primal~\eqref{eq-primal-unb}. 
One has $\pi_1 = m(\be)\al$ and $\pi_2=m(\al)\be$.
Since $(m(\al),m(\be))\in \text{\upshape dom}(\phi)$, it yields
\begin{align*}
  \OTb(\al,\be) \leq \dotp{\al\otimes\be}{\C} + m(\al)\phi(m(\be)) + m(\be)\phi(m(\al)).
\end{align*}
%
Second, let us focus on the dual formulation~\eqref{eq-dual-unb}. 
Consider constant potentials $(\f^*,\g^*)$ such that $-\f^* \in\nabla\phi(m(\be))$ and $-\g^* \in\nabla\phi(m(\al))$
They satisfy the optimality condition~\ref{eq-dual-optimality} when $\epsilon\rightarrow\infty$. 
Such $(\f^*,\g^*)$ exist because $(m(\al),m(\be))\in \text{\upshape dom}(\phi)$, thus $\partial\phi\neq\emptyset$. 
This is equivalent to 
\begin{align}
  \phi(m(\be)) = -\f^* m(\be) - \phi^*(-\f^*)\qandq%\label{eq-suboptim-cond-legendre1},
%  -\g^* \in\nabla\phi(m(\al))=\arg\max_x x m(\al) - \phi(x).
  \phi(m(\al)) = -\g^* m(\al) - \phi^*(-\g^*).\label{eq-suboptim-cond-legendre}
\end{align}
By suboptimality of $(\f^*,\g^*)$ we have $\OTb(\al,\be)\geq\Ff(\f^*,\g^*)$. 
It holds for any $\epsilon>0$, thus at the limit $\epsilon\rightarrow\infty$, a Taylor expansion of $\epsilon (e^{(\f\oplus\g-\C)/\epsilon} - 1)$ yields
\begin{align}
  &\lim_{\epsilon\rightarrow\infty} \OTb(\al,\be)
   	\geq \dotp{\al}{-\phi^*(-\f^*)} + \dotp{\be}{-\phi^*(-\g^*)} + \dotp{\al\otimes\be}{\C - (\f^*\oplus\g^*)} \label{eq-asymptotic-dual1}\\
  &\qquad\geq \dotp{\al\otimes\be}{\C} + m(\al)\big(-\f^* m(\be) - \phi^*(-\f^*)\big) + m(\be)\big(-\g^* m(\al) - \phi^*(-\g^*)\big)\label{eq-asymptotic-dual2}\\
  &\qquad\geq \dotp{\al\otimes\be}{\C} + m(\al)\phi(m(\be)) + m(\be)\phi(m(\al)).\label{eq-asymptotic-dual3}
\end{align}
Equation~\eqref{eq-asymptotic-dual2} is a simplification of Equation~\eqref{eq-asymptotic-dual1} because the potentials $(\f^*,\g^*)$ are constant. 
Equation~\eqref{eq-asymptotic-dual3} applies Equation~\eqref{eq-suboptim-cond-legendre}.
We have $\lim_{\epsilon\rightarrow\infty}\OTb(\al,\be)$.
Summing all the terms of the Sinkhorn divergence gives the second formula.
\end{proof}

This result shows that $\Sb(\al,\be)$ diverges as $\epsilon \rightarrow +\infty$ when $m(\al) \neq m(\be)$.
Note that this proof avoids $\Gamma$-convergence arguments.
For balanced OT one would take (not constant) $\f^*=\C\star\be - \tfrac{1}{2}\dotp{\al\otimes\be}{\C}$ and $\g^*=\C\star\al - \tfrac{1}{2}\dotp{\al\otimes\be}{\C}$.

\subsection{Positive definiteness of the Sinkhorn divergence}
\label{sec-pos-sink-div}

We present now the main results of this section on $\Sb$.

\begin{theorem}[The Sinkhorn divergence $\Sb$ is positive, definite and convex] \label{thm-sink-unb}
  Assume $\C$ is symmetric, $\gamma$-Lipschitz, and that $k_\epsilon = e^{- \C / \epsilon}$ is a positive universal kernel. 
  For any $\epsilon >0$, for any entropy $\phi$, the Sinkhorn divergence $\Sb(\al,\be)$ is positive, definite and convex in $\al$ and $\be$ (but not jointly).
\end{theorem}
\begin{proof}
The kernel $k_\epsilon$ is positive, thus it defines a positive kernel norm. 
Applying Proposition~\ref{prop-Seps-ineq-norm}, we get that $\forall(\al,\be)\in\Mmp(\Xx),\, \Sb(\al,\be)\geq 0$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The function $(\al,\be)\mapsto(m(\al) - m(\be))^2$ is convex,  $\Fb$ is convex (Theorem~\ref{thm-Feps_uniqueness}) and $\OTb$ is convex in each of its inputs (Theorem~\ref{thm-continuity-unb}). 
Summing everything proves $\Sb$ is convex in $\al$ and in $\be$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Proving definiteness holds with Propositions~\ref{prop-Seps-ineq-norm} and Lemma~\ref{lem-injective-sym-pot}. 
If $\Sb(\al,\be)=0$, then so is the kernel norm. 
Since $k_\epsilon$ is universal, we get $\al e^{\f_{\al} / \epsilon} = \be e^{\g_{\be} / \epsilon}$, which implies that $\al=\be$ thanks to Lemma~\ref{lem-injective-sym-pot}.
\end{proof}

This last theorem focuses on properties of $\Sb$ with respect to the weak* topology when $\D_\phi=\rho\KL$ or $\rho\TV$.
While taking such $\D_\phi$ seems restrictive, they are the two settings most frequently studied in the litterature.

\begin{theorem}[The Sinkhorn divergence $\Sb$ metrizes the convergence in law]
	\label{thm-sink-weak-cv}
When $\D_\phi = \rho\KL$ or $\rho\TV$,
$\Sb$ metrizes the convergence in law: for any sequence $(\al_n)_n$ in
$\Mmpp(\Xx)$, we have $\al_n\rightharpoonup\al \Longleftrightarrow \Sb(\al_n,\al) \rightarrow 0.$
\end{theorem}
\begin{proof}
Assume $\al_n\rightharpoonup\al$. 
Theorem~\ref{thm-diff-unb} and Proposition~\ref{prop-uniform-conv} gives that $\Sb$ is weak* continuous.
By definition $\Sb(\al,\al)=0$, thus $\Sb(\al_n,\al)\rightarrow 0$.

Conversely, Assume $\Sb(\al_n,\al)\rightarrow 0$. 
Assume $(m(\al_n))_n$ is uniformly bounded.s
Since $\Xx$ is compact, Banach-Alaoglu theorem gives $(\al_n)$ is a compact sequence.
Take any weak limit $\al_{n_\infty}$ of a subsequence $(\al_{n_k})_k$.
By continuity $\Sb(\al_{n_\infty},\al)=0$, and definiteness implies $\al_{n_\infty}=\al$
Thus $(\al_n)$ has a unique limit and converges to $\al$.

It remains to prove $(m(\al_n))_n$ is uniformly bounded. 
Write $\bar{\al}_n = \al_n / m(\al_n)$. 
When $\D_\phi = \rho\KL$, noting $\f_n$ and $\bar{\f}_n$ optimal potentials of $\Fb(\al_n)$ and $\Fb(\bar{\al}_n)$.
Linking optimality conditions of $(\al_n, \bar{\al}_n)$, one obtains the relation $\f_n = \bar{\f}_n -\tfrac{\rho\epsilon}{2\rho + \epsilon}\log(m(\al_n))$.

Since $\Sb(\al_n,\al)\rightarrow 0$, Proposition~\ref{prop-Seps-ineq-norm} gives that $\norms{\al_n e^{\f_n / \epsilon}}_{k_\epsilon}\rightarrow \norms{\al e^{\f / \epsilon}}_{k_\epsilon}$ where $\f$ is optimal for $\Fb(\al)$. 
The optimality condition of $\f_n$ reads $e^{-\f_n / \rho}=e^{\f_n / \epsilon}\dotp{\al_n}{e^{(\f_n - \C) / \epsilon}}$. 
Thus we reformulate the kernel norm as
\begin{align*}
    \norm{\al_n e^{\f_n / \epsilon}}^2_{k_\epsilon} = \dotp{\al_n\otimes\al_n}{e^{(\f_n\oplus\f_n-\C)/\epsilon}}
    = \dotp{\al_n}{e^{-\f_n / \rho}}
     =\dotp{\bar{\al}_n}{e^{-\bar{\f}_n / \rho}} m(\al_n)^{\frac{\epsilon}{2\rho + \epsilon} + 1}.
\end{align*}
The sequence $\norms{\al_n e^{\f_n / \epsilon}}_{k_\epsilon}$ converges, so it is bounded. 
If $m(\al_n)\rightarrow\infty$, it imposes that $\dotp{\bar{\al}_n}{e^{-\bar{\f}_n} / \rho}$ converges to $0$.
 Since $\bar{\al}_n$ is a probability on a compact space, it imposes $\norm{\bar{\f}_n}_\infty\rightarrow\infty$, which contradicts the coercivity of $\Ff$ and the optimality of $\bar{\f}_n$ since $\Fb(\bar{\al}_n)>\infty$.
For $\D_\phi=\rho\TV$, $\aprox{\phi^*}$ imposes $\f_n\geq -\rho$, thus one has $M > \norms{\al_n e^{\f_n / \epsilon}}_{k_\epsilon} \geq e^{(-2\rho-\text{diam}(\Xx))/\epsilon}m(\al_n)^2$.
In both cases the mass is necessarily bounded, which ends the proof on the weak* metrization of $\Sb$.
\end{proof}

\subsection{Case of the null measure}
\label{sec-null-meas}

The case $\al=0$ needs to be treated separately because dual potentials lack regularity. 
Indeed, If $\al=0$ then $\al\otimes\be=0$ and the regularization $\KL(.,\al\otimes\be)$ imposes that the only feasible plan is $\pi=0$.
Thus the primal cost is equal to $\OTb(\al,\be) = m(\be) \phi(0)$. 
Note that we assume $\phi(0) < +\infty$, otherwise $\OTb(0,\be)=+\infty$. 
In that case the primal is well-defined with an explicit formula. 
Concerning the dual, it reads $\OTb(\al=0,\be) = \sup_{\g\in\Cc(\Xx)} \dotp{\be}{-\phi^*(-\g)}$.
When $\D_\phi = \KL$, the dual program is equal to the primal, but the sup is not attained in $\Cc(\Xx)$ because $\g=+\infty$ is optimal.
Thus, we cannot use the regularity of dual potentials given in Proposition~\ref{prop-uniform-conv} to prove the regularity of OT when any of the input measures is null.
Nevertheless, it is possible to prove via the primal that OT functionals are regular when the input measures go to zero.

\begin{proposition}[Continuity of unbalanced OT at the null measure]
Assume $\phi$ is a continuous entropy with $\text{\upshape dom}(\phi) = \R_+$. 
Take $(\al_n,\be_n)\rightharpoonup(0,\be)$ with $\be\in\Mmp(\Xx)$. 
Then $\OTb$ is weak* continuous at $(0,\be)$, $\Fb$ is weak* continuous and $\Sb$ is weak* continuous and positive at $(0,\be)$ under the assumptions of Theorem~\ref{thm-sink-unb}.
\end{proposition}
\begin{proof}
The plan $\pi_n = \al_n\otimes\be_n$ is feasible (since $\text{dom}(\phi) = \R_+$) and suboptimal. 
It yields an upper bound on $\OTb$. 
Jensen inequality on $\D_\phi$ (which is also positive) gives a lower bound. 
They read
\begin{align*}
  \OTb(\al_n,\be_n) &\geq  \inf_{\pi\in\Mmp(\Xx^2)} \dotp{\pi}{\C} + m(\al_n)\phi( \frac{m(\pi_{n})}{m(\al_n)} ) + m(\be_n)\phi( \frac{m(\pi_{n})}{m(\be_n)} ),\\
  \OTb(\al_n,\be_n) &\leq  \dotp{\al_n\otimes\be_n}{\C} + m(\al_n)\phi(m(\be_n)) + m(\be_n)\phi(m(\al_n)).
\end{align*}
The lower bound is an infimum on a lower semicontinuous functional and is thus bounded from below by the infimun of the limit $\al_n\rightharpoonup 0$ and $\be_n\rightharpoonup 0$. 
Thus $\pi=0$, since other plans yield an infinite cost, and $\OTb(\al_n,\be_n)\geq m(\be) \phi(0)$. 
The upper bound gives at the limit $\OTb(\al_n,\be_n)\rightarrow m(\be) \phi(0) = \OTb(0,\be)$ (because $\phi$ is continuous), which proves the weak* continuity of $\OTb$.
Concerning $\Fb$, the same proof holds using the suboptimal plan $\pi=\al_n\otimes\al_n$.
The Sinkhorn divergence $\Sb$ is positive for strictly positive measures and weak* continuous as a sum of weak* continuous functions. Thus when $\al_n\rightharpoonup 0 $ the positivity remains at the limit.
\end{proof}


\subsection{Extensions}
We detail here extensions of the theory we developped above.
They were not considered for the sake of simplicity, but should be worth considering for applications.
We provide motivated examples for such ideas, with details on how our theory should be adapted.

\subsubsection{Assymetric marginal penalties}
As suggested Remark~\ref{rem-asym-entropy}, one could want to consider penalties $\D_{\phi_1}(\pi_1|\al)$ and $\D_{\phi_2}(\pi_2|\be)$ with $\phi_1\neq\phi_2$.
For instance, take $\D_{\phi_1}=\iota_{(=)}$ and $\D_{\phi_2}=\rho\KL$.
It is relevant in e.g. domain adaptation where $\al$ is a source dataset on which a predictor was trained, and $\be$ is a similar but shifted dataset on which we want to transfer the learned predictor.

In this setting it is possible to define a Sinkhorn divergence which would be positive, but no longer symmetric.
It then reads
\begin{align*}
	\Sb^{(\phi_1,\phi_2)}(\al,\be) &= \OTb^{(\phi_1,\phi_2)}(\al,\be) - \OTb^{(\phi_1,\phi_1)}(\al,\al) - \OTb^{(\phi_2,\phi_2)}(\be,\be) 
	+ \tfrac{\epsilon}{2}(m(\al) - m(\be))^2,
\end{align*}
where $\OTb^{(\phi_1,\phi_2)}$ is the regularized OT program penalized with $(\D_{\phi_1},\D_{\phi_2})$.
Using the above formula, it is straightforward to prove Proposition~\ref{prop-Seps-ineq-norm}, hence the positivity of $\Sb^{(\phi_1,\phi_2)}$.
To compute $\OTb^{(\phi_1,\phi_2)}$ the only change is to consider two operators $\aprox{\phi^*_i}$ ($i\in\{1,2\}$) such that optimal potentials satisfy $\f = -\aprox{\phi^*_1}(-\Ss_\be(\g))$ and $\g = -\aprox{\phi^*_2}(-\Ss_\al(\f))$.


\subsubsection{Spatially varying $\phi$-divergences}
%For the sake of simplicity, we restrict our attention to spatially homogeneous divergences, so that $\phi$ does not depend on the position $x \in \Xx$. 
Recall CsiszÃ r divergence integrate pointwise penalties on $\tfrac{\d\al}{\d\be}$.
It is thus possible to generalize $\D_\phi$ as
\begin{align}\label{eq-csiszar-div-varying}
\D_\phi(\al|\be) \eqdef \int_\Xx \phi\big(\frac{\d\al}{\d\be}(x),x\big) \d\be +  \int_\Xx \phi^\prime_\infty(x) \d\al^\bot(x),
\end{align}
where $\phi(\cdot,x)$ is an entropy function for each location $x \in \Xx$ with associated recession value  $\phi^\prime_\infty(x)$.
%
Some regularity is however required to avoid measurability issues and be able to apply Legrendre duality.
It is well-defined when the function (defined on $\mathbb{R}_+^2 \times \Xx$) $\Phi : (r,s,x) \mapsto \phi(r/s,x) s$
(properly extended when $s=0$ using $\phi^\prime_\infty(x)$) is a so-called normal-integrant~\cite[chap.14]{rockafellar2009variational}.
%
For instance, this is ensured if $\Phi$ is lower-semi-continuous.


A typical example of such divergence consists in using a spatially varying parameter $\rho(x)$, such that for e.g. $\KL$ penalties one takes $\phi(p,x)=\rho(x) ( p \log p -p +1)$.
It allows to modulate the strength of the conservation of mass constraint over the spatial domain $\Xx$.
Such situation appears e.g. in biology where the frequency of cell duplications $\rho(x)$ depends on the cell functionality $x$.


Concerning computations, one takes a spatially varying map $\aprox{\phi^*(\cdot,x)}$ at each $x\in\Xx$.
Note that when $\phi(p,x)=\rho(x)\phi(p)$ one has $\phi^*(p,x) = \rho(x)\phi^*(p / \rho(x))$.
The full Sinkhorn update outputs the function $x\mapsto -\aprox{\phi^*(\cdot,x)}(-\Ss_\al(\f)(x))$, and for $\KL$ penalties mentioned above, it reads $\aprox{\phi^*(\cdot,x)}(q) = (1 + \tfrac{\epsilon}{\rho(x)})^{-1}q$.


%\begin{remark}[Extension to spatially varying divergences]
%	When considering spatially varying divergences of the form~\eqref{eq-csiszar-div-varying}, iterations~\eqref{eq-sinkhorn-iter-1} should be understood as using a spatially varying operator $\aprox{\phi^*}$. This means that for some function $f(x)$, one should apply the aprox operator of $\phi(\cdot,x)^*$ independently at each location $x$.
%	\begin{equation*}
%	\aprox{\phi^*}(f) : x \mapsto \aprox{\phi(\cdot,x)^*}(f(x)).
%	\end{equation*}
%	For instance, in the case of a spatially varying KL divergence defined using~\eqref{eq-space-varying-entropy}, one has
%	\begin{equation*}
%	\aprox{\phi^*}(f) : x \mapsto (1+\tfrac{\epsilon}{\rho(x)})^{-1} f(x).
%	\end{equation*}
%\end{remark}



