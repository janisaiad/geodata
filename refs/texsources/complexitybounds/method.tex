% !TEX root = ../main.tex
\section{Efficient Optimization of MTW}\label{sec:optim}
For the sake of convenience, we denote by bold symbols sequences of the form $\boldsymbol{Z} = (z^1, \dots, z^T)$.
%
\paragraph{Loss function.}
We solve MTW by alternating minimization on the positive and negative parts of the regression coefficients $\boldsymbol\theta$ and those of $\thetabar$. Let $\bs{P}_1$ and $\bs{P}_2$ denote respectively the optimal transport plans linking $\bs\theta_+$ with $\thetabar_+$ and $\bs\theta_-$ with $\thetabar_-$, and $\bs{m}_1$ and $\bs{m}_2$ their respective left marginals. Combining \eqref{eq:mtw}, \eqref{eq:H} and \eqref{eq:uw},  the cost function to minimize is given by:
%
\begin{equation}
\label{eq:mtw-loss}
\begin{aligned}
 L(\btheta; \boldsymbol{P_1}; \boldsymbol{P_2}; \thetabar) & = \sum_{t=1}^T \Big[ \frac{1}{2n} \| X^t \theta^t - Y^t \|^2 + \frac{\lambda}{T} \|\theta^t \|_1 \\ & G(P_1^t, \theta_+^t, \thetabar_+) + G(P_2^t, \theta_-^t, \thetabar_-) \Big] \enspace .
\end{aligned}
\end{equation}
%
$L$ is jointly convex in all its variables (since the Kullback-Leibler is jointly convex) and the remaining terms are not coupled.
The straightforward solution is to minimize $L$ by block coordinate descent.

Since the minimization with respect to the groups $(P_1^t, \theta_+^t, \thetabar_+)$ and $P_2^t, \theta_-^t, \thetabar_-)$ is similar, we only detail hereafter the minimization with respect to $(P_1^t, \theta_+^t, \thetabar_+)$. The full optimization strategy is provided in Algorithm~\ref{alg:alt-opt}. 
We alternate with respect to $(\boldsymbol{P_1}, \thetabar_+)$ and each $\theta_+^t$, which can be updated independently and therefore in parallel.
We now detail the two steps of the procedure.
%
\paragraph{Barycenter update.}
%
For fixed $\boldsymbol\theta_+$, minimizing with respect to $\boldsymbol P_1, \thetabar_+$ boils down to an unbalanced Wasserstein 
barycenter computation of ~\citep{chizat:17} which generalizes previous work by \citet{agueh:11} to compute the minimizer of
%
$\min_{\thetabar_+ \in \bbR_+^p} \frac{1}{T} \sum_{t=1}^T W(\theta_+^t, \thetabar_+)$. This is equivalent to minimizing simultaneously in $P_1^1, \dots, P_1^t \in \bbR_+^{p \times p}$ and $\thetabar_+ \in \bbR_+^p$ the objective:
\begin{equation}
    \label{eq:ubar}\varepsilon\! \sum_{t=1}^T \kl(P_1^t, K) + \gamma \!\!\sum_{t=1}^T \kl(P_1^t\mathds 1 | \theta_+^t)  +  \gamma \!\!\sum_{t=1}^T \kl({P_1^t}^\top \mathds 1| \thetabar_+) \enspace .
\end{equation}
%
As pointed by \citet{chizat:17} and recalled in~\eqref{eq:uw-dual}, Fenchel-Rockafellar duality allows to minimize over dual variables $u^t, v^t \in \bbR^p$ instead of $P_1^t \in \bbR_+^{p \times p}$. Each optimal plan $P_1^t$ is then given by $(u^t_i K_{ij} v^t_j)_{ij}$ and its left marginal, needed for the coefficient update, is given by $m_1^t = P_1^t\mathds 1 = u^t\odot Kv^t$. These steps are summarized in Algorithm~\ref{alg:sinkhorn}. As a stopping criterion, we monitor the largest relative change of the barycenter $\thetabar_+$.

\paragraph{Coefficients update.} 
Minimizing with respect to one $\theta_+^t$ while keeping all other variables fixed to their current estimate yields problem~\eqref{eq:mtw_coef}, where the $\ell_1$ penalty becomes linear due to the positivity constraint. Given the left marginal $m_1$, the problem reads for all $\theta_+^t$ (omitting index $t$):
%
\begin{equation}
\label{eq:mtw_coef}
\begin{aligned}
\min_{\theta_+ \in \bbR^p_{++} } \Big[&\frac{1}{2n} \|X\theta_+ - X\theta_- - Y\|^2 + \\ & \sum_{i=1}^p \frac{\mu \gamma}{T} ({\theta_+}_i - m_i \log({\theta_+}_i)) + \lambda {\theta_+}_i \Big]\enspace.
\end{aligned}
\end{equation}
%
The penalty is a separable sum of convex and functions with tractable proximal operators. This implies that~\eqref{eq:mtw_coef} can be solved by proximal coordinate descent~\citep{Tseng01,fercoq}. The following proposition gives a closed-from solution to the proximal operator.
%
\begin{prop}
	\label{prop:prox} Let $a, b, \alpha \in \bbR_{++}$. The function $g: x \mapsto  (x - a \log(x)) + b x$ is convex on $\bbR_{++}$, moreover its proximal operator can be obtained in closed form and is given by:
	\[
	\prox_{\alpha g}(y) = \frac{1}{2}\left[ - \alpha(b + 1)  + y + \sqrt{(\alpha(b + 1) - y)^2 + 4 \alpha a} \right]\enspace .
	\]
\end{prop}
\proof See appendix. \qed
% The proof is defered in the appendix.
\begin{algorithm}[h]
   \caption{Alternating optimization}
   \label{alg:alt-opt}
\begin{algorithmic}
   \STATE {\bfseries Input:}  $ \theta^0, a^0, b$, hyperparameters: $\mu, \epsilon , \gamma, \lambda$ and cost matrix $M$.
      \STATE {\bfseries Output:} $\btheta$, the minimizer of \eqref{eq:mtw}.

   \REPEAT
   \FOR{$t=1$ {\bfseries to} $T$}
    \STATE Update $\theta_+^t$ with proximal coordinate descent.
       \STATE Update $\theta_-^t$ with proximal coordinate descent.
      \ENDFOR
    \STATE Update the left marginals $m_+^1, \dots, m_+^t$  and $ \thetabar_+$ with generalized Sinkhorn.
       \STATE Update the left marginals $m_-^1, \dots, m_-^t$  and $ \thetabar_-$ with generalized Sinkhorn.
       \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
	\caption{Generalized Sinkhorn\citep{chizat:17}}
	\label{alg:sinkhorn}
	\begin{algorithmic}
		\STATE {\bfseries Input:}  $ \theta^1, \dots, \theta^T$
         \STATE {\bfseries Output:} Wasserstein barycenter of $ \theta^1, \dots, \theta^T$ and marginals $m^1, \dots, m^T$.
		\STATE Initialize for $(t = 1, \dots, T) \, (u^t, v^t) = (\mathds 1, \mathds 1)$, 
		\REPEAT
		\FOR{$t=1$ {\bfseries to} $T$}
		\STATE $u^t \gets \left(\theta^t/Kv^t\right)^{\frac{\gamma}{\gamma + \varepsilon}}$
		\ENDFOR
		\STATE $\thetabar \gets \left( \tfrac{1}{T}\sum_{t=1}^T (v \odot K^\top u^t) ^{ \frac{\varepsilon}{\varepsilon + \gamma} }\right)^{\frac{\varepsilon + \gamma}{\varepsilon}} $
		\FOR{$t=1$ {\bfseries to} $T$}
		\STATE $v^t \gets \left(\thetabar/K^\top u^t\right)^{\frac{\gamma}{\gamma + \varepsilon}}$
		\ENDFOR
		\UNTIL{convergence}
 \FOR{$t=1$ {\bfseries to} $T$}
	\STATE $m^t = u^t \odot K v^t$
	\ENDFOR
	\end{algorithmic}
\end{algorithm}

%\begin{prop}
%	\label{prop:plan}
%	\citep{chizat:17}
%	
%	Let $P^\star$ denote the minimizer of \eqref{eq:uw}.  Fenchel-Rockafellar duality theorem states:
%	\begin{equation}
%	\label{eq:plan}
%	\left(\exists u, v \in \bbR^p_+  \right) \quad P^\star  = (u \otimes v) \odot K
%	\end{equation}
%	where $K = e^{- M / \varepsilon}$ and $u \otimes v$ denotes the matrix $(u_i  v_j)_{(i, j)} \in \bbR_+^{p \times p}$. 
%\end{prop}

\paragraph{Entropy regularization.}
While large values of $\varepsilon$ (strong entropy regularization) induce undesired blurring, low values tend to cause a well-documented numerical instability~\citep{chizat:17, schmitzer16}, which can be avoided by moving to the log-domain~\citep{schmitzer16}. Also for experiments performed on regular grids such as images, one can leverage the separability of the kernel $K$ as proposed in \citep{solomon:15}. This also applies to log-domain computations~\citep{schmitz:17}.

\paragraph{Accelerating convergence with warm-start.}
To speed up convergence, we initialize the Sinkhorn scaling vectors to their previous values that we keep in memory between two barycenter computations. Note that this does not affect convergence due to the convexity of the objective function.
Also note that the explicit instantiation of the transport plans $P^1, \dots, P^T$ is never needed. We only compute their left marginals $m^1, \dots, m^T$ that come into play in the coefficients update. We set the global termination criterion to a tolerance on the relative evolution of both the objective function decrease and that of the norm of the coefficients. We observe that performing less Sinkhorn iterations per barycenter update speeds up considerably the convergence, while reaching the same final tolerance threshold. See supplementary materials for an illustration of this tradeoff and more details on Sinkhorn iterations.


\paragraph{Hyperparameter tuning.}
The MTW model has four hyperparameters: $\epsilon, \gamma, \mu, \lambda$, which is a lot in practice when one needs to compute a cross-validation on a full grid of parameters. Yet, we now explain how this issue can be mitigated by detailing strategies to fix the parameters $\varepsilon$ and $\gamma$ in the Wasserstein distance.

\textit{Setting $\varepsilon$.}
As mentioned above, we set the entropy weight $\varepsilon$ as small as possible, in our experiments we observe that a value of $1/sp$, where $s$ is the median of the ground metric $M$, provides an excellent tradeoff between speed and performance. 

\textit{Setting $\gamma$.}
In the barycenter definition ~\eqref{eq:ubar}, $\gamma$ controls the influence of the marginals. To find an appropriate value, we adopt the following strategy. When $\gamma \to 0$, variable $P$ tends to $K$ since we can ignore marginal constraints. Such a transport plan, however, only leads to a local blur with no transportation of mass, so that the mass of the corresponding barycenter $\thetabar \mathds 1 \to 0.$ To avoid this degenerate behavior, consider the case where $\gamma \gg \varepsilon$ so entropy regularization can be neglected in \eqref{eq:ubar}. The corresponding approximate objective function is given by:
\begin{equation}
    \label{eq:ubar-noreg}
    \sum_{t=1}^T \left[  \langle P^t, M \rangle  +  \gamma \kl(P^t\mathds 1 | a^t)  +  \gamma  \kl(P{^t}^\top \mathds 1| a) \right] \enspace .
\end{equation}
By deriving the first order conditions, we get for any $t \in \intset{T}$:
%
\begin{equation*}
\label{eq:gamma1}
M_{ij} + \gamma \log\left(\frac{P^t_{i.} P^t_{.j}}{a^t_i\bar{a}_j}\right) = 0\quad \text{ and }\quad  \bar{a} = \frac{1}{T}\sum_{t=1}^T {P^t}^\top \mathds 1
\end{equation*}
By combining the two, we get for any $\tau \in [0, 1]$:
\begin{equation}
    \label{eq:heuristic}
  \gamma \geq -\frac{\max{M}}{\log{\tau}} \Rightarrow  \bar{\psi} \geq \tau \left(\frac{1}{T} \sum_{t=1}^T \sqrt{\psi_t} \right)^2 \enspace ,
\end{equation}
%
where $\bar{\psi}, \psi_1, \dots, \psi_t$ denote respectively the masses $\bar{a} \mathds 1, a^1 \mathds 1, \dots, a^T \mathds 1$ i.e $\psi^t = {a^t}^\top \mathds 1$. Therefore, ~\eqref{eq:heuristic} provides an adaptive parametrization of $\gamma$ that guarantees a lower bound on the mass of $\thetabar$ as a fraction of the $\ell_{0.5}$ pseudo-norm of those of the inputs. In practice, in all experiments we use $\tau = 0.5$ and set $\gamma = \tau \left(\frac{1}{T} \sum_{t=1}^T \sqrt{\psi_t} \right)^2$.

With $\varepsilon$ and $\gamma$ fixed, remains only two hyperparameters $(\mu, \lambda)$ which control respectively the similarity between tasks and the sparsity. Setting two parameters is not more than what is required by Dirty models, the multi-level Lasso or an Elastic-Net.
