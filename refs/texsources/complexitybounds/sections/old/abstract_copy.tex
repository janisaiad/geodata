\begin{abstract}
Optimal transport (OT) and maximum mean discrepancies (MMD) have emerged in recent years as the most robust tools to compare probability measures in a machine learning context. A few years back, entropy-regularized optimal transport was introduced to alleviate the computational cost of OT. The peuso-distance induced by the latter, called Sinkhorn divergence, exhibits interesting properties in experimental settings, performing well for stuctured tasks where OT is effective but avoiding overfitting contrarily to standard OT which suffers from the curse of dimensionality.  It was shown recently that it in fact interpolates between OT and MMD. In this paper, we exhibit theoretical properties of Sinkhorn divergences, shedding some light on these empirical observations. We \emph{(i)} derive a bound on the approximation error when using Sinkhorn divergences to approximate OT, as a function of the regularization parameter, \emph{(ii)} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and \emph{(iii)} give a sample complexity result for Sinkhorn divergences. For the latter, the key is to rewrite Sinkhorn divergences as a maximization problem in a reproducing kernel Hilbert space, and thus benefit from sample complexity results similar to maximum mean discrepancies, scaling in $\frac{1}{\sqrt{n}}$ but with a constant that depends on the inverse of the regularizisation parameter. 
\end{abstract}