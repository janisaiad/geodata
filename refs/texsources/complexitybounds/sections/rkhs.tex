\section{Properties of Sinkhorn Potentials}

We prove in this section that Sinkhorn potentials are bounded in the Sobolev space $\mathbf H^s(\RR^d)$ regardless of the marginals $\alpha$ and $\beta$. For $s>\frac{d}{2}$, $\mathbf H^s(\RR^d)$ is a reproducing kernel Hilbert space (RKHS): This property will be crucial to establish sample complexity results later on, using standard tools from RKHS theory.

\begin{defn} The Sobolev space $\mathbf H^s(\Xx)$, for $s \in \NN^*$, is the space of functions $\phi : \Xx \subseteq \RR^d \rightarrow \RR$ such that for every multi-index $k$ with $\abs{k} \leq s$ the mixed partial derivative $\phi^{(k)}$ exists and belongs to $L^2(\Xx)$. It is endowed with the following inner-product
\eql{ \langle \phi, \psi \rangle _{\mathbf H^s(\Xx)} = \sum_{\abs{k} \leq s} \int_{\Xx} \phi^{(k)}(x) \psi^{(k)}(x) \d x .
}
\end{defn}

\begin{thm}\label{thm_rkhs}
When $\Xx$ and $\Yy$ are two compact sets of $\RR^d$ and the cost $c$ is $\Cc^\infty$, then the Sinkhorn potentials $(u,v)$ are uniformly bounded in the Sobolev space $\mathbf H^s(\RR^d)$ and their norms satisfy
\eq{
\norm{u}_{\mathbf H^s} = O\left(1+ \frac{1}{\epsilon^{s-1}}\right) \; \text{and} \;  \norm{v}_{\mathbf H^s}= O\left(1+\frac{1}{\epsilon^{s-1}}\right),} 
with constants that only depend on $\abs{\Xx}$ (or $\abs{\Yy}$ for $v$),$d$, and $\norminf{c^{(k)}}$ for $k = 0 ,\dots,  s$.
In particular, we get the following asymptotic behavior in $\epsilon$: $\norm{u}_{\mathbf H^s} = O(1)$  as $\epsilon \rightarrow +\infty$  and $\norm{u}_{\mathbf H^s} = O(\frac{1}{\epsilon^{s-1}})$ as $\epsilon \rightarrow 0$.

%\qandq \norm{v}_{\mathbf H^s} = O\left(\max(1,\frac{1}{\epsilon^{s-1}})\right) .}
\end{thm}

To prove this theorem, we first need to state some regularity properties of the Sinkhorn potentials.
\begin{prop} \label{prop_potentials}
If $\Xx$ and $\Yy$ are two compact sets of $\RR^d$ and the cost $c$ is $\Cc^\infty$, then
\begin{itemize}
\item $u(x)\in [\min_y v(y) - c(x,y), \max_y v(y) - c(x,y)]$ for all $x\in \Xx$
\item $u$ is L-Lipschitz, where L is the Lipschitz constant of $c$
\item $u \in \Cc^\infty(\Xx)$ and $\norminf{u^{(k)}} =  O (1 + \frac{1}{\epsilon^{k-1}})$
\end{itemize}
and the same results also stand for $v$ (inverting $u$ and $v$ in the first item, and replacing $\Xx$ by $\Yy$).
\end{prop}


\begin{proof}
The proofs of all three claims exploit the optimality condition of the dual problem:
\eql{\label{optim_condition}
\!\!\!\exp\left(\frac{-u(x)}{\epsilon}\right) = \int \exp\left(\frac{v(y)-c(x,y)}{\epsilon}\right)\beta(y) \d y.
}
Since $\beta$ is a probability measure, $e^{\frac{-u(x)}{\epsilon}}$ is a convex combination of $\phi : x \mapsto e^{\frac{v(x)-c(x,y)}{\epsilon}}$ and thus $e^{\frac{-u(x)}{\epsilon}} \in [ \min_y \phi (y) , \max_y \phi (y)].$ We get the desired bounds by taking the logarithm. The two other points use the following lemmas:

\begin{lem}\label{lem_pot_1}
The derivatives of the potentials are given by the following recurrence
\begin{equation}
u^{(n)}(x) =  \int g_n(x,y) \gamma_\epsilon(x,y) \beta(y) \d y,
\end{equation}
where $$g_{n+1} (x,y) = g_n' (x,y) + \frac{u'(x)-c'(x,y)}{\epsilon} g_n (x,y) ,$$ $g_1(x,y) = c'(x,y)$
and $\gamma_\epsilon(x,y) = \exp(\frac{u(x)+v(y)-c(x,y)}{\epsilon})$.
\end{lem}

\begin{lem}\label{lem_pot_2} The sequence of auxiliary functions $(g_k)_{k=0\dots}$ verifies
 $\norminf{u^{(k)}} \leq \norminf{g_k}$. Besides,  for all $j=0,\dots,k$, for all $k=0,\dots,n-2$, $\norminf{g_{n-k}^{(j)}}$ is bounded by a polynomial in $\frac{1}{\epsilon}$ of order $n-k+j-1$.
\end{lem}

The detailed proofs of the lemmas can be found in the appendix. We give here a sketch in the case where $d=1$. Lemma~\ref{lem_pot_1} is obtained by a simple recurrence, consisting in differentiating both sides of the dual optimality condition. Differentiating under the integral is justified with the usual domination theorem, bounding the integrand thanks to the Lipschitz assumption on $c$, and this bound is integrable thanks to the marginal constraint. Differentiating once and rearranging terms gives:
\begin{equation} \label{uprim}
u'(x) = \int c'(x,y) \gamma_\epsilon(x,y) \beta(y) dy.
\end{equation}
where $\gamma_\epsilon$ is defined in Lemma~\ref{lem_pot_1}. One can easily see that $\gamma_\epsilon'(x,y) = \frac{u'(x)-c'(x,y)}{\epsilon} \gamma_\epsilon(x,y)$ and this allows to conclude the recurrence, by differentiating both sides of the equality. From the primal constaint, we have that $ \int_\Yy  \gamma_\epsilon(x,y) \beta(y) \d y = 1$. Thus thanks to Lemma~\ref{lem_pot_1} we immediately get that $\norminf{u^{(n)}} \leq \norminf{g_n}$. For $n=1$, since $g_1 = c'$ we get that $\norminf{u'} = \norminf{c'} = L$ and this proves the second point of Proposition~\ref{prop_potentials}. For higher values of $n$, we need the result from Lemma~\ref{lem_pot_2}. This property is also proved by recurrence, but requires a bit more work. To prove the induction step, we need to go from bounds on $g_{n-k}^{(i)}$, for $k=0,\dots,n-2$ and $i=0,\dots,k$ to bounds on $g_{n+1-k}^{(i)}$, for $k=0,\dots,n-1$ and $i=0,\dots,k$. 
Hence only new quantities that we need to bound are  $g_{n+1-k}^{(k)}, k=0,\dots,n-1$. This is done by another (backwards) recurrence on $k$ which involves some tedious computations, based on Leibniz formula, that are detailed in the appendix.
\end{proof}

Combining the bounds of the derivatives of the potentials with the definition of the norm in $\mathbf H^s$, is enough to complete the proof of Theorem~\ref{thm_rkhs}.
\begin{proof}(Theorem~\ref{thm_rkhs}) The norm of $u$ in $\mathbf H^s(\Xx)$ is
\eq{\norm{u}_{\mathbf H^s} =  \left( \sum_{\abs{k} \leq s} \int_\Xx (u^{(k)})^2  \right)^\frac{1}{2} \leq \abs{\Xx} \left(\sum_{\abs{k} \leq s} \norminf{u^{(k)}}^2 \right)^\frac{1}{2}.}
From Proposition~\ref{prop_potentials} we have that $\forall k, \norminf{u^{(k)}} = O (1+\frac{1}{\epsilon^{k-1}})$ and thus we get that $\norm{u}_{\mathbf H^s} = O (1+\frac{1}{\epsilon^{s-1}})$.
We just proved the bound in $\mathbf H^s(\Xx)$ but we actually want to have a bound on $\mathbf H^s(\RR^d)$. This is immediate thanks to the Sobolev extension theorem \citep{calderon1961lebesgue} which guarantees that $\norm{u}_{\mathbf H^s(\RR^d)} \leq C \norm{u}_{\mathbf H^s(\Xx)}$ under the assumption that $\Xx$ is a bounded Lipschitz domain. 
\end{proof}

This result, aside from proving useful in the next section to obtain sample complexity results on the Sinkhorn divergence, also proves that kernel-SGD can be used to solve continuous regularized OT. This idea introduced in \cite{2016-genevay-nips} consists in assuming the potentials are in the ball of a certain RKHS, to write them as a linear combination of kernel functions and then perform stochastic gradient descent on these coefficients. Knowing the radius of the ball and the kernel associated with the RKHS (here the Sobolev or Mat√©rn kernel) is crucial to obtain good numerical performance and ensure the convergence of the algorithm.

