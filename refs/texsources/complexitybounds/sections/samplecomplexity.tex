\section{Approximation from Samples}

In practice, measures $\alpha$ and $\beta$ are only known through a finite number of samples. Thus, what can be actually computed in practice is the Sinkhorn divergence between the empirical measures $\hat \alpha_n \eqdef \frac{1}{n} \sumin \delta_{X_i}$ and $\hat \beta_n \eqdef \frac{1}{n} \sumin \delta_{Y_i}$, where $(X_1,\dots,X_n)$ and $(Y_1,\dots,Y_n)$ are n-samples from $\alpha$ and $\beta$, that is
\begin{multline*}
W_\epsilon(\hat\alpha_n,\hat\beta_n)  =  \max_{u,v} \sumin u(X_i) + \sumin v(Y_i) \\
- \epsilon \sumin \exp\left(\frac {u(X_i)+v(Y_i) - c(X_i,Y_i)}{\epsilon}\right) + \epsilon \\
 =   \max_{u,v} \frac{1}{n}\sumin f_\epsilon^{X_iY_i} (u,v) + \epsilon,
\end{multline*}
where $(X_i,Y_i)_{i=1}^n$ are i.i.d random variables distributed according to $\alpha\otimes\beta$. On actual samples, these quantities can be computed using Sinkhorn's algorithm~\citep{CuturiSinkhorn}.

Our goal is to quantify the error that is made by approximating $\alpha,\beta$ by their empirical counterparts $\hat \alpha_n,\hat \beta_n$, that is bounding the following quantity:
\begin{align}
\vert W_\epsilon(\alpha,\beta) &- W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert =  \nonumber\\
&\vert  \EE f_\epsilon^{XY}(u^*,v^*) - \frac 1 n  \sumin f_\epsilon^{X_iY_i} (\hat u , \hat v) \vert ,  \label{sup_bound}
\end{align}
where $(u^*,v^*)$ are the optimal Sinkhorn potentials associated with $(\alpha,\beta)$ and $(\hat u , \hat v)$ are their empirical counterparts.

\begin{thm} Consider the Sinkhorn divergence between two measures $\alpha$ and $\beta$ on $\Xx$ and $\Yy$ two bounded subsets of $\RR^d$, with a $\Cc^\infty$, $L$-Lipschitz cost $c$. One has 
\eq{\EE\vert W_\epsilon(\alpha,\beta) - W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert  =   O \left(\frac{e^\frac{\kappa}{\epsilon}}{\sqrt{n}}\left(1+\frac{1}{\epsilon^{\lfloor d/2 \rfloor }}\right) \right) }
where $\kappa = 2 L \abs{\Xx}+\norminf{c}$ and constants only depend on $\abs{\Xx}$,$\abs{\Yy}$,$d$, and $\norminf{c^{(k)}}$ for $k = 0 \dots \lfloor d/2 \rfloor$. In particular, we get the following asymptotic behavior in $\epsilon$: 
\begin{align*}
\EE\vert W_\epsilon(\alpha,\beta) - W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert  &=& O\left(\frac{e^\frac{\kappa}{\epsilon}}{\epsilon^{\lfloor d/2 \rfloor }\sqrt{n}}\right) \text{ as }\epsilon \rightarrow 0 \\
\EE\vert W_\epsilon(\alpha,\beta) - W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert  &=& O\left(\frac{1}{\sqrt{n}}\right)\qquad\text{ as }\epsilon \rightarrow + \infty.
\end{align*}
\label{sample_complexity_thm}
\end{thm}

An interesting feature from this theorem is the fact when $\epsilon$ is large enough, the convergence rate does not depend on $\epsilon$ anymore. This means that at some point, increasing $\epsilon$ will not substantially improve convergence. However, for small values of $\epsilon$ the dependence is critical.

We prove this result in the rest of this section. The main idea is to exploit standard results from PAC-learning in RKHS. Our theorem is an application of the following result from \cite{bartlett2002rademacher} ( combining Theorem~12,4) and Lemma~22 in their paper):
\begin{prop}{(Bartlett-Mendelson '02)} \label{prop:Bartlett}
Consider $\alpha$ a  probability distribution, $\ell$ a B-lipschitz loss and $\Gg$ a given class of functions. Then
$$
\EE_\alpha \left[ \sup_{g \in \Gg} \EE_\alpha \ell(g,X) - \frac 1 n \sumin \ell(g,X_i) \right] \leq 2 B \EE_\alpha \Rr(\Gg(X_1^n))
$$
where $\Rr(\Gg(X_1^n))$ is the Rademacher complexity of class $\Gg$ defined by $ \Rr(\Gg(X_1^n))=\sup_{g \in \Gg} \EE_\sigma \frac 1 n  \sumin \sigma_i g(X_i)$ where $(\sigma_i)_i$ are iid Rademacher random variables.
Besides, when $\Gg$ is a ball of radius $\lambda$ in a RKHS with kernel $k$ the Rademacher complexity is bounded by
$$
\Rr(\Gg_\lambda(X_1^n)) \leq \frac \lambda n \sqrt{\sumin k(X_i,X_i)}.
$$
\end{prop}

Our problem falls in this framework thanks to the following lemma:
\begin{lem} \label{lem_bound} Let $\Hh^s_\lambda \eqdef \{ u \in \mathbf H^s(\RR^d) \mid \norm{u}_{\mathbf H^s(\RR^d)} \leq \lambda \}$, then there exists $\lambda$ such that:
\begin{align*}
\vert W_\epsilon(\alpha,\beta) &- W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert \leq  \\
&3  \sup_{(u,v) \in (\Hh^s_\lambda)^2}\vert \EE f_\epsilon^{XY}(u,v) -\frac 1 n   \sumin f_\epsilon^{X_iY_i} (u, v) \vert .
\end{align*} 
\end{lem}

\begin{proof} Inserting  $\EE f_\epsilon^{XY}(\hat u,\hat v)$ and using the triangle inequality in \eqref{sup_bound} gives
\begin{align*}
	& \vert W_\epsilon(\alpha,\beta) \!-\! W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert \leq \vert  \EE f_\epsilon^{XY}(u^*,v^*) \!-\! \EE f_\epsilon^{XY}(\hat u,\hat v)   \vert \\  
	 &\quad + \vert   \EE f_\epsilon^{XY}(\hat u,\hat v) -\frac 1 n   \sumin f_\epsilon^{X_iY_i} (\hat u, \hat v)   \vert .
\end{align*}
From Theorem~\ref{thm_rkhs}, we know that the all the dual potentials are bounded in $\mathbf H^s(\RR^d)$ by a constant $\lambda$ which doesn't depend on the measures. Thus the second term is bounded by $\sup_{(u,v) \in (\Hh^s_\lambda)^2} \vert \EE f_\epsilon(u,v) -\frac 1 n   \sumin f_\epsilon (u, v) \vert $ .\\
The first quantity needs to be broken down further. Notice that it is non-negative since $(u^*,v^*)$ is the maximizer of $\EE f_\epsilon(\cdot,\cdot)$  so we can leave out the absolute value. We have:
\begin{align}
\EE f_\epsilon^{XY}&(u^*,v^*) - \EE f_\epsilon^{XY}(\hat u,\hat v)   \leq  \nonumber\\
&\phantom{+}\EE f_\epsilon^{XY}(u^*,v^*) - \frac 1 n   \sumin f_\epsilon^{X_iY_i} (u^*,v^*)  \label{A}\\
& +\frac 1 n   \sumin f_\epsilon^{X_iY_i} (u^*,v^*)  - \frac 1 n   \sumin f_\epsilon^{X_iY_i} (\hat u, \hat v)  \label{B}\\
&+  \frac 1 n   \sumin f_\epsilon^{X_iY_i} (\hat u, \hat v) - \EE f_\epsilon^{XY}(\hat u,\hat v)  \label{C}
\end{align}
Both \eqref{A} and \eqref{C} can be bounded by $\sup_{(u,v) \in (\Hh^s_\lambda)^2} \vert \EE f_\epsilon^{XY}(u,v) -\frac 1 n   \sumin f_\epsilon^{X_iY_i}  (u, v) \vert $ while \eqref{B} is non-positive since $(\hat u,\hat v)$ is the maximizer of $\frac 1 n   \sumin f_\epsilon^{X_iY_i}  (\cdot , \cdot)$.
\end{proof}

To apply Proposition~\ref{prop:Bartlett} to Sinkhorn divergences we need to prove that \emph{(a)} the optimal potentials are in a RKHS and \emph{(b)} our loss function $f^\epsilon$ is Lipschitz in the potentials.

The first point has already been proved in the previous section. The RKHS we are considering is $\mathbf H^s(\RR^d)$ with $s=\lfloor\frac{d}{2} \rfloor+1$. It remains to prove that $f^\epsilon$ is Lipschitz in $(u,v)$ on a certain subspace that contains the optimal potentials.


 \begin{figure*}[h]
  \centering
  \includegraphics[width=.32\textwidth]{W2_d2.png} 
  \includegraphics[width=.32\textwidth]{W2_d3.png}  
  \includegraphics[width=.32\textwidth]{W2_d7.png} 
  \caption{$\bar W_\epsilon(\hat\alpha_n,\hat\alpha_n')$ as a function of $n$ in log-log space : Influence of $\epsilon$ for fixed $d$ on two uniform distributions on the hypercube with quadratic cost.} \label{fig:d}
  \vspace*{-10pt}
  \end{figure*}

 \begin{figure*}[h]
  \centering
  \includegraphics[width=.32\textwidth]{W2_eps100.png} 
  \includegraphics[width=.32\textwidth]{W2_eps1.png}  
  \includegraphics[width=.32\textwidth]{W2_eps001.png} 
  \caption{$\bar W_\epsilon(\hat\alpha_n,\hat\alpha_n')$ as a function of $n$ in log-log space : Influence of $d$ for fixed $\epsilon$ on two uniform distributions on the hypercube with quadratic cost.} \label{fig:epsilon}
    \vspace*{-10pt}
  \end{figure*}

\begin{lem}\label{lem_lipschitz}
Let $\Aa = \{ (u,v) \mid  u \oplus v  \leq 2 L \abs{\Xx} + \norminf{c} \}$. We have:
\begin{itemize}
\item[(i)] the pairs of optimal potentials $(u^*,v^*)$ such that $u^*(0)=0$ belong to $\Aa$,
\item[(ii)] $f^\epsilon$ is B-Lipschitz in $(u,v)$ on $\Aa$ with $B \leq 1 + \exp(2\frac{L \abs{\Xx} + \norminf{c}}{\epsilon})$. 
\end{itemize}
\end{lem}
\begin{proof}
Let us prove that we can restrict ourselves to a subspace on which $f^\epsilon$ is Lipschitz in $(u,v)$.
$$f^\epsilon (u,v,x,y) = u(x) + v(y) - \epsilon  \exp\left(\frac {u(x)+v(y) - c(x,y)}{\epsilon}\right)$$
$$\nabla f^\epsilon (u,v) = 1 - \exp\left(\frac{u+v-c}{\epsilon}\right).$$

To ensure that $f^\epsilon$ is Lipschitz, we simply need to ensure that the quantity inside the exponential is upperbounded at optimality and then restrict the function to all $(u,v)$ that satisfy that bound.

Recall the bounds on the optimal potentials from Proposition~\ref{prop_potentials}. We have that $\forall x \in \Xx, y \in \Yy$,  
\eq{u(x)\leq L \abs{x} \quad \text{and} \quad v(y) \leq \max_x u(x) - c(x,y).}
Since we assumed $\Xx$ to be a bounded set, denoting by $\abs{\Xx}$ the diameter of the space we get that at optimality
$\forall x \in \Xx, y \in \Yy$
\eq{u(x) + v(y)  \leq 2 L \abs{\Xx} + \norminf{c}.}

Let us denote  $\Aa = \{ (u,v) \in (\mathbf{H}^s(\RR^d))^2 \mid  u \oplus v  \leq 2 L \abs{\Xx} + \norminf{c} \}$, we have that $\forall (u,v) \in \Aa$, 
\[\abs{\nabla f^\epsilon (u,v) }\leq 1 + \exp(2 \frac{L \abs{\Xx} + \norminf{c}}{\epsilon}).\qedhere\]
\end{proof} 


We now have all the required elements to prove our sample complexity result on the Sinkhorn loss, by applying Proposition~\ref{prop:Bartlett}.

\begin{proof} (Theorem~\ref{sample_complexity_thm})
Since $f_\epsilon$ is Lipschitz and we are optimizing over $\mathbf H^s(\RR^d)$ which is a RKHS, we can apply Proposition~\ref{prop:Bartlett} to bound the $\sup$ in  Lemma~\ref{lem_bound}. We get:
\eq{\EE \vert W_\epsilon(\alpha,\beta) - W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert \leq 3  \frac { 2 B \lambda} {n} \EE \sqrt{\sumin  k(X_i,X_i)}}
where $B \leq 1 + \exp(2\frac{L \abs{\Xx} + \norminf{c}}{\epsilon})$ (Lemma~\ref{lem_lipschitz}),
 $\lambda = O(\max(1,\frac{1}{\epsilon^{d/2}}))$ (Theorem~\ref{thm_rkhs}).
We can further bound $\sqrt{\sumin k(X_i,X_i)}$ by $\sqrt{n \max_{x \in \Xx} k(x,x)}$ where $k$ is the kernel associated to $H^s(\RR^d)$ (usually called Matern or Sobolev kernel) and thus $\max_{x \in \Xx} k(x,x) = k(0,0):= K$ which doesn't depend on $n$ or $\epsilon$.
Combining all these bounds, we get the convergence rate in $\frac{1}{\sqrt{n}}$ with different asymptotic behaviors in $\epsilon$ when it is large or small.
\end{proof}

Using similar arguments, we can also derive a concentration result:
\begin{cor}
With probability at least $1-\delta$,
\eq{\vert W_\epsilon(\alpha,\beta) - W_\epsilon(\hat \alpha_n, \hat \beta_n) \vert  \leq 6 B \frac{\lambda K}{\sqrt{n}} + C \sqrt{\frac{2 \log \frac{1}{\delta}}{n}}}
where $B,\lambda,K$ are defined in the proof above, and $C = \kappa + \epsilon \exp(\frac{\kappa}{\epsilon})$ with $\kappa = 2 L \abs{\Xx}+\norminf{c}$.
\label{cor:concentration}
\end{cor}

\begin{proof}
We apply the bounded differences (Mc Diarmid) inequality to $g: (x_1,\dots,x_n) \mapsto \sup_{u,v \in \Hh_\lambda^s}(\EE f_\epsilon^{XY} - \frac{1}{n}f_\epsilon^{X_i,Y_i})$. From Lemma~\ref{lem_lipschitz} we get that $\forall x,y$, $f_\epsilon^{xy}(u,v) \leq \kappa + \epsilon e^{\kappa/\epsilon}\eqdef C$, and thus, changing one of the variables in $g$ changes the value of the function by at most $2C/n$. Thus the bounded differences inequality gives 
\eq{
\PP\left(\abs{g(X_1,\dots ,X_n) - \EE g(X_1,\dots ,X_n)} >t \right) \leq 2 \exp(\frac{t^2n}{2C^2})}
Choosing $t  = C\sqrt{\frac{2 \log\frac{1}{\delta}}{n}}$ yields that with probability at least $1-\delta$
\eq{ g(X_1,\dots ,X_n) \leq \EE g(X_1,\dots ,X_n) + C \sqrt{\frac{2 \log \frac{1}{\delta}}{n}}
}
and from Theorem~\ref{sample_complexity_thm} we already have
\eq{\EE g(X_1,\dots ,X_n) = \EE \sup_{u,v \in \Hh_\lambda^s}(\EE f_\epsilon^{XY} - \frac{1}{n}f_\epsilon^{X_i,Y_i}) \leq  \frac { 2 B \lambda K} {\sqrt{n}}.}\end{proof}
%\begin{align*}
%\sup_{u,v}& \vert \EE f_\epsilon^{XY} (u,v) -\frac 1 n   \sumin f_\epsilon^{X_iY_i} (u, v) \vert \leq \\
%&\EE \left[\sup_{u,v} \vert \EE f_\epsilon^{XY} (u,v) -\frac 1 n   \sumin f_\epsilon^{X_iY_i}  (u, v) \vert \right] + \sqrt{\frac{2 \log \frac{1}{\delta}}{n}}
%\end{align*}


\input{sections/numerics}

