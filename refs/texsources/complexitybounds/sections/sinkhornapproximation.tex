\section{Approximating Optimal Transport with Sinkhorn Divergences}

In the present section, we are interested in bounding the error made when approximating $W(\alpha,\beta)$ with $W_\epsilon(\alpha,\beta)$. 

\begin{thm}
Let $\alpha$ and $\beta$ be probability measures on $\Xx$ and $\Yy$ subsets of $\mathbb{R}^d$ such that $\abs{\Xx} = \abs{\Yy} \leq D$ and assume that $c$ is $L$-Lipschitz w.r.t.\ $x$ and $y$. It holds
\begin{eqnarray}
0 \leq W_\epsilon(\alpha,\beta) - W(\alpha,\beta) &\leq  2 \epsilon d \log\left(\frac{e^2\cdot L\cdot D}{\sqrt{d}\cdot \epsilon}\right) \\  &\sim_{\epsilon \to 0 } 2\epsilon d \log(1/\epsilon) .
\end{eqnarray}
\end{thm}

\begin{proof}
For a probability measure $\pi$ on $\Xx\times \Yy$, we denote by $C(\pi)=\int c \, \d \pi$ the associated transport cost and by $H(\pi)$ its relative entropy  with respect to the product measure $\alpha \otimes \beta$ as defined in \eqref{entropy}.
%For a regularization parameter $\epsilon>0$, we denote by $\pi_\epsilon$ the minimizer of 
%\[\min_{\pi \in \Pi(\alpha,\beta)} C(\pi) + \epsilon H(\pi)\]
%which is unique by strict convexity of the functional. 
Choosing $\pi_0$ a minimizer of $\min_{\pi \in \Pi(\alpha,\beta)} C(\pi)$, we will build our upper bounds using a family of transport plans with finite entropy that approximate $\pi_0$. The simplest approach consists in considering block approximation. In contrast to the work of~\citet{Carlier2017}, who also considered this technique, our focus here is on quantitative bounds.

\begin{defn}[Block approximation]
For a resolution $\Delta>0$, we consider the block partition of $\RR^d$ in hypercubes of side $\Delta$ defined as 
\begin{eqnarray*}
 \{ Q^\Delta_k = {[k_1\cdot \Delta,(k_1+1)\cdot \Delta[} \times \dots {[k_d\cdot \Delta,(k_d +1)\cdot \Delta[}\; ;\\
  k = (k_1,\dots,k_d) \in \mathbb{Z}^d\}.
\end{eqnarray*}

To simplify notations, we introduce $Q^\Delta_{ij}\eqdef Q^\Delta_i \times Q^\Delta_j$, $\boldsymbol{\alpha}_i^\Delta \eqdef \alpha(Q_i^\Delta)$, $\boldsymbol{\beta}_j^\Delta \eqdef \beta(Q_j^\Delta)$.
The block approximation of $\pi_0$ of resolution $\Delta$ is the measure $\pi^\Delta \in \Pi(\alpha,\beta)$ characterized by
\[
\pi^\Delta\vert_{Q^\Delta_{ij}} =  \frac{\pi_0(Q^\Delta_{ij})}{\boldsymbol{\alpha}_i^\Delta \cdot \boldsymbol{\beta}_j^\Delta } (\alpha\vert_{Q^\Delta_i}\otimes \beta\vert_{Q^\Delta_j})
\]
for all $(i,j) \in (\mathbb{Z}^d)^2$, with the convention $0/0=0$.
\end{defn}

$\pi^\Delta$ is nonnegative by construction. Observe also that for any Borel set $B\subset \RR^d$, one has
\begin{align*}
\pi^\Delta(B\times \RR^d) &= \sum_{(i,j)\in (\mathbb{Z}^d)^2} \frac{\pi_0(Q_{ij}^\Delta)}{\boldsymbol{\alpha}_i^\Delta \cdot \boldsymbol{\beta}_j^\Delta } \cdot \alpha(B\cap Q^\Delta_i) \cdot \boldsymbol{\beta}_j^\Delta \\
&= \sum_{i \in \mathbb Z^d} \alpha(B\cap Q^\Delta_i) =  \alpha(B),
\end{align*}
which proves, using the symmetric result in $\beta$, that $\pi^\Delta$ belongs to $\Pi(\alpha,\beta)$. 
As a consequence, for any $\epsilon>0$ one has $W_\epsilon(\alpha,\beta)  \leq C(\pi^\Delta) + \epsilon H(\pi^\Delta)$.
Recalling also that the relative entropy $H$ is nonnegative over the set of probability measures, we have the bound
\[
0\leq W_\epsilon(\alpha,\beta) - W(\alpha,\beta) \leq (C(\pi^\Delta)-C(\pi_0)) + \epsilon H(\pi^\Delta).
\]
We can now bound the terms in the right-hand side, and choose a value for $\Delta$ that minimizes these bounds.

The bound on $C(\pi^\Delta)-C(\pi_0)$ relies on the Lipschitz regularity of the cost function. Using the fact that $\pi^\Delta(Q^\Delta_{ij})=\pi_0(Q^\Delta_{ij})$ for all $i,j$, it holds
\begin{align*}
C(\pi^\Delta)-C(\pi_0) %&\leq \sum_{(i,j)\in (\mathbb{Z}^d)^2}  \left[ \sup_{x,y \in Q^\Delta_{ij}} c(x,y) \pi^\Delta(Q^\Delta_{ij}) - \inf_{x,y \in Q^\Delta_{ij}} c(x,y) \pi_0(Q^\Delta_{ij}) \right] \\
& = \sum_{(i,j)\in (\mathbb{Z}^d)^2}  \pi_0(Q^\Delta_{ij}) \Big( \sup_{x,y \in Q^\Delta_{ij}} c(x,y) \\  & \phantom{\sum_{(i,j)\in (\mathbb{Z}^d)^2}  \pi_0(Q^\Delta_{ij}) \Big(} - \inf_{x,y \in Q^\Delta_{ij}} c(x,y) \Big)\\
&\leq 2L\Delta \sqrt{d},
\end{align*}
where $L$ is the Lipschitz constant of the cost (separately in $x$ and $y$) and $\Delta\sqrt{d}$ is the diameter of each set $Q^\Delta_{i}$.

As for the bound on $H(\pi^\Delta)$, using the fact that $\pi_0(Q^\Delta_{ij})\leq 1$ we get
\begin{align*}
H(\pi^\Delta) &= \! \! \sum_{(i,j)\in (\mathbb{Z}^d)^2}  \! \log\left(\frac{ \pi_0(Q^\Delta_{ij})}{\boldsymbol{\alpha}_i^\Delta\cdot \boldsymbol{\beta}_j^\Delta} \right) \pi_0(Q^\Delta_{ij}) \\
& \leq \sum_{(i,j)\in (\mathbb{Z}^d)^2} \left( \log(1/\boldsymbol{\alpha}_i^\Delta) + \log(1/ \boldsymbol{\beta}_j^\Delta) \right)  \pi_0(Q^\Delta_{ij})\\
& = - H^\Delta(\alpha) - H^\Delta(\beta),
%& = \sum_{i \in \mathbb{Z}^d} \boldsymbol{\alpha}_i^\Delta\log(1/\boldsymbol{\alpha}_i^\Delta)  +  \sum_{j \in \mathbb{Z}^d} \boldsymbol{\beta}_j^\Delta\log(1/\boldsymbol{\beta}_j^\Delta) \\
%& \leq 2d\cdot \log(1/\Delta) +H_{\mathcal{L}^d}(\alpha^{\Delta}) +H_{\mathcal{L}^d}(\beta^{\Delta})
\end{align*}
where we have defined $H^\Delta(\alpha) =\sum_{i \in \mathbb{Z}^d} \boldsymbol{\alpha}_i^\Delta\log(\boldsymbol{\alpha}_i^\Delta)$ and similarly for $\beta$. Note that in case $\alpha$ is a discrete measure with finite support, $H^\Delta(\alpha)$ is equal to (minus) the discrete entropy of $\alpha$ as long as $\Delta$ is smaller than the minimum separation between atoms of $\alpha$. However, if $\alpha$ is not discrete then $H^\Delta(\alpha)$ blows up to $-\infty$ as $\Delta$ goes to $0$ and we need to control how fast it does so.
%
Considering $\alpha^\Delta$ the block approximation of $\alpha$ with constant density $\boldsymbol{\alpha}_i^\Delta/\Delta^d$ on each block $Q^\Delta_i$ and (minus) its differential entropy $H_{\mathcal{L}^d}(\alpha^\Delta) = \int_{\mathbb{R}^d} \alpha^\Delta(x) \log \alpha^\Delta(x) dx$, it holds 
$ H^\Delta(\alpha) = H_{\mathcal{L}^d}(\alpha^{\Delta}) - d\cdot \log(1/\Delta)$. Moreover, using the convexity of $H_{\mathcal{L}^d}$, this can be compared with the differential entropy of the uniform probability on a hypercube containing $\Xx$ of size $2D$. Thus it holds $H_{\mathcal{L}^d}(\alpha^{\Delta}) \geq - d \log (2D)$ and thus $H^\Delta(\alpha) \geq - d \cdot \log(2D/\Delta)$.

Summing up, we have for all $\Delta>0$
\[
W_\epsilon(\alpha,\beta) - W(\alpha,\beta) \leq  2L \Delta \sqrt{d} + 2\epsilon d\cdot \log(2D/\Delta) .
\]
The above bound is convex in $\Delta$, minimized with $\Delta = 2\sqrt{d}\cdot \epsilon/L$. This yields
\[
W_\epsilon(\alpha,\beta) - W(\alpha,\beta) \leq 4\epsilon d + 2 \epsilon d \log\left(\frac{L\cdot D}{\sqrt{d}\cdot \epsilon}\right).\qedhere
\]

\end{proof}